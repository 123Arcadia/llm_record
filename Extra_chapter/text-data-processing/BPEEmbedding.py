import re
from collections import Counter, defaultdict
from typing import List, Tuple, Dict

import tiktoken
from pycparser.ply.yacc import token
from sympy.physics.secondquant import wicks


class SampleBPETokenizer:
    """
    ç®€å•å®ç°BPE
    """

    def __init__(self, vocab_size: int = 100):
        """
        åˆå§‹åŒ–
        :param vocab_size: è¯æ±‡è¡¨å¤§å°
        """
        self.vocab_size = vocab_size
        self.vocab = {}  # è¯æ±‡è¡¨: æ ‡è®° -> ID
        self.reverse_vocab = {}
        self.bpe_ranks = {}  # bpeåˆå¹¶è§„åˆ™ä¼˜å…ˆçº§
        self.special_tokens = {
            "<pad>": 0,
            "<unk>": 1,
            "<bos>": 2,
            "<eos>": 3,
        }
        self.vocab_size_actual = len(self.special_tokens)

        for token, idx in self.special_tokens.items():
            self.vocab[token] = idx
            self.reverse_vocab[idx] = token

    def build_vocab(self, texts: List[str]):
        """
        æ„å»ºè¯æ±‡è¡¨
        :param texts:
        :return:
        """
        # åˆå§‹åŒ–è¯æ±‡è¡¨ï¼ŒåŒ…å«æ‰€æœ‰çš„å•å­—ç¬¦
        token_counts = Counter()
        # token_counts_2 = Counter()
        for text in texts:
            words = [' '.join(list(text))]
            # print(f'{words=}')
            for w in words:
                # token_counts_2.update(w)
                token_counts[w] += 1

        """
        è¿™ä¸¤ä¸ªç»“æœçš„åŒºåˆ«ï¼š
        words=['H e l l o ,   h o w   a r e   y o u   t o d a y ?']
        words=['I   h o p e   y o u   a r e   d o i n g   w e l l .']
        words=['T h i s   i s   a   t e s t   o f   t h e   B P E   t o k e n i z e r .']
        wordséƒ½æ˜¯ä¸€ä¸ªlistï¼ŒåŒ…å«ä¸€ä¸ªå­—ç¬¦ä¸²
        Counter().update()æ–¹æ³•æ˜¯æ¥æ”¶ä¸€ä¸ªã€å¯è¿­ä»£å¯¹è±¡ã€‘æ¥ç»Ÿè®¡å­—ç¬¦å‡ºç°æ¬¡æ•°
        Counter()[w]æ˜¯æŠŠå­—ç¬¦ä¸²å½“æˆä¸€ä¸ªæ•´å¯¹è±¡æ¥ç»Ÿè®¡çš„ï¼Œå®ƒä¸ä¼šè¿›ä¸€æ­¥åˆ’åˆ†å­—ç¬¦
        
        """
        # print(f'{token_counts=}')
        # token_counts=Counter({'H e l l o ,   h o w   a r e   y o u   t o d a y ?': 1, 'I   h o p e   y o u   a r e   d o i n g   w e l l .': 1, 'T h i s   i s   a   t e s t   o f   t h e   B P E   t o k e n i z e r .': 1})
        # print(f'{token_counts_2=}')
        # token_counts_2=Counter({' ': 100, 'e': 9, 'o': 9, 't': 5, 'l': 4, 'h': 4, 'a': 4, 'i': 4, 'r': 3, 'y': 3, 's': 3, 'w': 2, 'u': 2, 'd': 2, 'n': 2, '.': 2, 'H': 1, ',': 1, '?': 1, 'I': 1, 'p': 1, 'g': 1, 'T': 1, 'f': 1, 'B': 1, 'P': 1, 'E': 1, 'k': 1, 'z': 1})
        # ç»Ÿè®¡åˆå§‹çš„å­—ç¬¦è¯æ±‡è¡¨
        chars = set()
        for text in texts:
            chars.update(text)

        # æ·»åŠ å­—ç¬¦åˆ°è¯æ±‡è¡¨
        for char in sorted(chars):
            if char not in self.vocab:
                self.vocab[char] = self.vocab_size_actual
                self.reverse_vocab[self.vocab_size_actual] = char
                self.vocab_size_actual += 1

        # BPEåˆå¹¶
        num_merges = self.vocab_size - self.vocab_size_actual
        # print(f'{token_counts=}\n{num_merges=}')
        # token_counts=Counter({' ': 100, 'e': 9, 'o': 9, 't': 5, 'l': 4, 'h': 4, 'a': 4, 'i': 4, 'r': 3, 'y': 3, 's': 3, 'w': 2, 'u': 2, 'd': 2, 'n': 2, '.': 2, 'H': 1, ',': 1, '?': 1, 'I': 1, 'p': 1, 'g': 1, 'T': 1, 'f': 1, 'B': 1, 'P': 1, 'E': 1, 'k': 1, 'z': 1})
        # num_merges=17
        # print(f'{self.vocab=}')
        # self.vocab={'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3, ' ': 4, ',': 5, '.': 6, '?': 7, 'B': 8, 'E': 9, 'H': 10, 'I': 11, 'P': 12, 'T': 13, 'a': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'k': 21, 'l': 22, 'n': 23, 'o': 24, 'p': 25, 'r': 26, 's': 27, 't': 28, 'u': 29, 'w': 30, 'y': 31, 'z': 32}
        # print(f'{self.vocab_size=}')
        # self.vocab_size=50
        # print(f'{self.vocab_size_actual=}')
        # self.vocab_size_actual=33
        # assert 0, "æš‚åœ!!!!"
        if num_merges <= 0:
            return

        pairs = token_counts.copy()
        # print(f'{pairs=}')
        # pairs=Counter({'H e l l o ,   h o w   a r e   y o u   t o d a y ?': 1, 'I   h o p e   y o u   a r e   d o i n g   w e l l .': 1, 'T h i s   i s   a   t e s t   o f   t h e   B P E   t o k e n i z e r .': 1})
        for i in range(num_merges):
            # è®¡ç®—æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡
            stats = self._get_stats(pairs)
            # i=0 stats=defaultdict(<class 'int'>, {('H', 'e'): 1, ('e', 'l'): 2, ('l', 'l'): 2, ('l', 'o'): 1, ('o', ','): 1, (',', 'h'): 1, ('h', 'o'): 2, ('o', 'w'): 1, ('w', 'a'): 1, ('a', 'r'): 2, ('r', 'e'): 2, ('e', 'y'): 2, ('y', 'o'): 2, ('o', 'u'): 2, ('u', 't'): 1, ('t', 'o'): 3, ('o', 'd'): 1, ('d', 'a'): 1, ('a', 'y'): 1, ('y', '?'): 1, ('I', 'h'): 1, ('o', 'p'): 1, ('p', 'e'): 1, ('u', 'a'): 1, ('e', 'd'): 1, ('d', 'o'): 1, ('o', 'i'): 1, ('i', 'n'): 1, ('n', 'g'): 1, ('g', 'w'): 1, ('w', 'e'): 1, ('l', '.'): 1, ('T', 'h'): 1, ('h', 'i'): 1, ('i', 's'): 2, ('s', 'i'): 1, ('s', 'a'): 1, ('a', 't'): 1, ('t', 'e'): 1, ('e', 's'): 1, ('s', 't'): 1, ('o', 'f'): 1, ('f', 't'): 1, ('t', 'h'): 1, ('h', 'e'): 1, ('e', 'B'): 1, ('B', 'P'): 1, ('P', 'E'): 1, ('E', 't'): 1, ('o', 'k'): 1, ('k', 'e'): 1, ('e', 'n'): 1, ('n', 'i'): 1, ('i', 'z'): 1, ('z', 'e'): 1, ('e', 'r'): 1, ('r', '.'): 1})
            if not stats:
                break

            # é€‰æ‹©æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹
            best = max(stats, key=stats.get)
            # è®°å½•åˆå¹¶è§„åˆ™çš„ä¼˜å…ˆçº§
            self.bpe_ranks[best] = i
            # åˆå¹¶è¯æ±‡è¡¨ä¸­çš„å­—èŠ‚å¯¹
            pairs = self._merge_vocab(best, pairs)
            # print(f'{best=}')
            # print(f'{pairs=}')

            # å°†æ–°åˆå¹¶çš„æ ‡è®°æ·»åŠ åˆ°è¯æ±‡è¡¨
            new_token = ''.join(best)
            # print(f'{new_token=}')
            # print(f'{self.vocab=}')
            # print(f'{self.vocab_size_actual=}')
            if new_token not in self.vocab:
                self.vocab[new_token] = self.vocab_size_actual
                self.reverse_vocab[self.vocab_size_actual] = new_token
                self.vocab_size_actual += 1

    def _get_stats(self, pairs: Dict[Tuple[str, str], int]) -> Dict[Tuple[str, str], int]:
        """è®¡ç®—æ‰€æœ‰ç›¸é‚»å­—ç¬¦çš„é¢‘ç‡"""
        stats = defaultdict(int)
        for word, freq in pairs.items():
            symbols = word.split()  # å¦‚æœæ˜¯å•ä¸ªå­—ç¬¦å°±æ²¡ç”¨
            for i in range(len(symbols) - 1):
                stats[symbols[i], symbols[i + 1]] += freq
        return stats

    def _merge_vocab(self, pair: Tuple[str, str], v_in: Dict[str, int]) -> Dict[str, int]:
        """
        åˆå¹¶æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹
        :param pair: æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹
        :param v_in: å·²å­˜åœ¨çš„å¯¹
        :return:
        """
        v_out = {}
        bigram = re.escape(' '.join(pair))
        p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
        for word in v_in:
            w_out = p.sub(''.join(pair), word)
            v_out[w_out] = v_in[word]
        return v_out

    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:
        """

        :param sample_text:
        :return:
        """
        tokens = self.tokenize(text)
        encoded = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]
        if add_special_tokens:
            encoded = [self.vocab["<bos>"]] + encoded + [self.vocab["<eos>"]]
        return encoded

    def tokenize(self, text: str) -> List[str]:
        """
        åˆ†è¯ä½œä¸ºBPEæ ‡è®°
        :param sample_text:
        :return:
        """
        tokens = []
        for token in text.split():
            if token in self.special_tokens:
                tokens.append(token)
            else:
                tokens.extend(self.bpe(token))
        return tokens

    def bpe(self, token: str) -> List[str]:
        """
        å¯¹å•ä¸ªåˆ†è¯ä½¿ç”¨bpeç®—æ³•
        :param text:
        :return:
        """
        if token in self.special_tokens:
            return [token]
        word = list(token)
        if len(word) == 0:
            return []
        if len(word) == 1:
            return [word[0]]

        pairs = self._get_pairs(word)

        while True:
            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜çš„å­—èŠ‚å¯¹
            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))
            if bigram not in self.bpe_ranks:
                break

            # bigramæ˜¯æœ€é¢‘ç¹çš„ç›¸é‚»å­—ç¬¦å¯¹ï¼ˆå­—èŠ‚å¯¹ï¼‰
            first, second = bigram
            new_word = []
            i = 0
            while i < len(word):
                try:
                    j = word.index(first, i)
                    new_word.extend(word[i:j]) # # å°† i åˆ° j ä¹‹é—´çš„å­—ç¬¦åŠ å…¥ new_word
                    i = j
                except:
                    # æœªæ‰¾åˆ° firstï¼Œå°†å‰©ä½™å­—ç¬¦åŠ å…¥ new_word å¹¶é€€å‡ºå¾ªç¯
                    new_word.extend(word[i:])
                    break
                # è‹¥ first åé¢ç´§è·Ÿ secondï¼Œåˆ™åˆå¹¶ä¸º first+second
                if i < len(word) - 1 and word[i + 1] == second:
                    new_word.append(first + second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
                #  æ›´æ–° word ä¸ºåˆå¹¶åçš„ç»“æœ
            word = new_word
            if len(word) == 1:
                break
            else:
                pairs = self._get_pairs(word)
        return word

    def _get_pairs(self, word: List[str]):
        """
        è·å–å•è¯ä¸­æ‰€æœ‰çš„ç›¸é‚»æ ‡è®°å¯¹
        :param word:
        :return:
        """
        pairs = set()
        prev_char = word[0]
        for char in word[1:]:
            pairs.add((prev_char, char))
            prev_char = char
        return pairs

    def decode(self, ids: List[int], remove_special_tokens: bool = False) -> List[str]:
        tokens=[]
        for idx in ids:
            if idx in self.reverse_vocab:
                token = self.reverse_vocab[idx]
                if remove_special_tokens and token in self.special_tokens:
                    continue
                tokens.append(token)
            else:
                tokens.append("<unk>")
        text = ''.join(tokens)
        return text



def test_tiktoken_bpe():
    """æµ‹è¯•tiktokenåº“çš„BPEåˆ†è¯å™¨"""
    print("\n=====æµ‹è¯•tiktoken BPEåˆ†è¯å™¨=====")
    # åˆå§‹åŒ–tiktoken bpeåˆ†è¯å™¨
    try:
        tokenizer = tiktoken.get_encoding("gpt2")
    except KeyError:
        tokenizer = tiktoken.get_encoding("cl100k_base")

    print(f'è¯æ±‡è¡¨å¤§å°:{tokenizer.n_vocab}')
    # æµ‹è¯•åˆ†è¯
    sample_text = "Hello, this is a test!"
    tokens = tokenizer.encode(sample_text)
    print(f'\nç¼–ç ç»“æœ(ID) : {tokens}')

    # è½¬æ¢IDä¸ºå­—èŠ‚
    token_types = [tokenizer.decode_single_token_bytes(token) for token in tokens]
    print(f'\nè§£ç ç»“æœ(å­—èŠ‚): {token_types}')

    # è§£ç 
    decoded = tokenizer.decode(tokens)
    print(f'\nè§£ç ç»“æœ: {decoded}')

    # æµ‹è¯•åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬
    unknown_text = "This is a unicorn ğŸ¦„ test."
    encoded_unknown = tokenizer.encode(unknown_text)
    decoded_unknown = tokenizer.decode(encoded_unknown)
    print(f'å«æœ‰æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: {unknown_text}')
    print(f'ç¼–ç : {encoded_unknown}')
    print(f'ç¼–è§£ç : {decoded_unknown}')

    # è®¡ç®—æ–‡æœ¬çš„tokensæ•°é‡
    print(f'\næ–‡æœ¬: "{sample_text}" çš„æ•°é‡æ˜¯ {len(tokens)}')
    # =====æµ‹è¯•tiktoken BPEåˆ†è¯å™¨=====
    # è¯æ±‡è¡¨å¤§å°:50257
    #
    # ç¼–ç ç»“æœ(ID) : [15496, 11, 428, 318, 257, 1332, 0]
    #
    # è§£ç ç»“æœ(å­—èŠ‚): [b'Hello', b',', b' this', b' is', b' a', b' test', b'!']
    #
    # è§£ç ç»“æœ: Hello, this is a test!
    # å«æœ‰æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: This is a unicorn ğŸ¦„ test.
    # ç¼–ç : [1212, 318, 257, 44986, 12520, 99, 226, 1332, 13]
    # ç¼–è§£ç : This is a unicorn ğŸ¦„ test.
    #
    # æ–‡æœ¬: "Hello, this is a test!" çš„æ•°é‡æ˜¯ 7


def test_simple_bpe_tokenizer():
    """æµ‹è¯•ç®€å•çš„BPEåˆ†è¯å™¨"""
    print("\n=====æµ‹è¯•ç®€å•çš„BPEåˆ†è¯å™¨=====")
    texts = [
        "Hello, how are you today?",
        "I hope you are doing well.",
        "This is a test of the BPE tokenizer."
    ]
    # åˆå§‹åŒ–BPEåˆ†è¯å™¨
    tokenizer = SampleBPETokenizer(vocab_size=50)

    # æ„å»ºè¯æ±‡è¡¨
    tokenizer.build_vocab(texts)
    print(f'è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size_actual}')
    print(f'å‰10ä¸ªè¯æ±‡é¡¹: {list(tokenizer.vocab.items())[:10]}')
    # print(f'{tokenizer.vocab=}')
    # print(f'{tokenizer.vocab_size_actual=}')
    # tokenizer.vocab={'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3, ' ': 4, ',': 5, '.': 6, '?': 7, 'B': 8, 'E': 9, 'H': 10, 'I': 11, 'P': 12, 'T': 13, 'a': 14, 'd': 15, 'e': 16, 'f': 17, 'g': 18, 'h': 19, 'i': 20, 'k': 21, 'l': 22, 'n': 23, 'o': 24, 'p': 25, 'r': 26, 's': 27, 't': 28, 'u': 29, 'w': 30, 'y': 31, 'z': 32, 'to': 33, 'el': 34, 'ell': 35, 'ho': 36, 'ar': 37, 'are': 38, 'yo': 39, 'you': 40, 'is': 41, 'Hell': 42, 'Hello': 43, 'Hello,': 44, 'Hello,ho': 45}
    # tokenizer.vocab_size_actual=46
    # assert  0, "æš‚åœ!!!"

    # æµ‹è¯•åˆ†è¯
    sample_text = "Hello, this is a test!"
    tokens = tokenizer.tokenize(sample_text)
    print(f'\nåˆ†è¯ç»“æœ: {tokens}')

    # æµ‹è¯•ç¼–ç 
    encoded = tokenizer.encode(sample_text, add_special_tokens=True)
    print(f'ç¼–ç ç»“æœ: {encoded}')

    # æµ‹è¯•è§£ç 
    decoded = tokenizer.decode(encoded, remove_special_tokens=True)
    print(f'è§£ç ç»“æœ: {decoded}')

    # æµ‹è¯•å«æœ‰æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬
    unknown_text = "This is a unicorn ğŸ¦„ test."
    encoded_unknown = tokenizer.encode(unknown_text)
    decoded_unknown = tokenizer.decode(encoded_unknown)
    print(f'\nåŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: {unknown_text}')
    print(f'ç¼–ç ç»“æœ: {encoded_unknown}')
    print(f'è§£ç ç»“æœ: {decoded_unknown}')

    print(f'æµ‹è¯•å…¨éƒ¨å®Œæˆ!')
    # =====æµ‹è¯•ç®€å•çš„BPEåˆ†è¯å™¨=====
    # è¯æ±‡è¡¨å¤§å°: 46
    # å‰10ä¸ªè¯æ±‡é¡¹: [('<pad>', 0), ('<unk>', 1), ('<bos>', 2), ('<eos>', 3), (' ', 4), (',', 5), ('.', 6), ('?', 7), ('B', 8), ('E', 9)]
    #
    # åˆ†è¯ç»“æœ: ['Hello,', 't', 'h', 'is', 'is', 'a', 't', 'e', 's', 't', '!']
    # ç¼–ç ç»“æœ: [2, 44, 28, 19, 41, 41, 14, 28, 16, 27, 28, 1, 3]
    # è§£ç ç»“æœ: Hello,thisisatest
    #
    # åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: This is a unicorn ğŸ¦„ test.
    # ç¼–ç ç»“æœ: [13, 19, 41, 41, 14, 29, 23, 20, 1, 24, 26, 23, 1, 28, 16, 27, 28, 6]
    # è§£ç ç»“æœ: Thisisauni<unk>orn<unk>test.
    # æµ‹è¯•å…¨éƒ¨å®Œæˆ!


if __name__ == '__main__':
    """
    å°†æœªè§è¿‡çš„å•è¯åˆ†è§£ä¸ºå­è¯å•å…ƒ
    """
    test_simple_bpe_tokenizer()
    test_tiktoken_bpe()
